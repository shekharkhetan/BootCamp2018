{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Problem Set 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sy\n",
    "from sympy import *\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import linalg \n",
    "max_line_width=np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.59046864, -11.41719028,  38.28498286]), -162.29194858820895)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(tol, x_0,b,Q):\n",
    "    iters = 0\n",
    "    error = 10\n",
    "    maxiter = 100\n",
    "    while error > tol and iters < maxiter:\n",
    "        Dft =  Q@x_0-b\n",
    "        α_num = Dft.T@Dft\n",
    "        α_den = Dft.T@Q@Dft\n",
    "        α = α_num/α_den\n",
    "        x_1 = x_0 - α*Dft\n",
    "        error =  linalg.norm(x_1-x_0,2)\n",
    "        iters = iters +1\n",
    "        x_0 = x_1\n",
    "        f_val = 0.5*(x_0.T@Q@x_0) - b.T@x_0\n",
    "    return x_0,f_val\n",
    "\n",
    "\n",
    "        \n",
    "A = np.random.random(9).reshape(3,3)\n",
    "Q = A@A.T\n",
    "b = np.array([10, 10,10])\n",
    "x_0 = np.array([3, 9,9])\n",
    "\n",
    "gradient_descent(1e-4,x_0,b,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(func,x_vec,Rerrf):\n",
    "    h = Rerrf**0.5\n",
    "    n= len(x_vec)\n",
    "    basis =  np.eye(n)\n",
    "    jac = np.zeros((n))\n",
    "    for j in range(n):\n",
    "        jac[j] = (1/h)*(func(x_vec+ h*basis[:,j])-func(x_vec))\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.2, 10. ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = np.array([[1., 2.], [3., 4.]])\n",
    "f = lambda x: (F @ x).T @ (F @ x)\n",
    "x = np.array([1.,1.])\n",
    "f(x)\n",
    "jacobian(f, x, .01)\n",
    "\n",
    "rosenbrock = lambda x: 100*(x[1]-x[0]**2)**2 + (1-x[0])**2\n",
    "\n",
    "jacobian(rosenbrock, x, .01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5899992 , 0.34651782])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def steepest_descent(f, x0, eps):\n",
    "    maxiter = 3*1e5\n",
    "    iters = 1\n",
    "    err =  1+eps\n",
    "    while err > eps and iters < maxiter:\n",
    "        Dfx = jacobian(f, x0, eps)\n",
    "        f_alpha_prime = lambda x: jacobian(f, x0 - x * Dfx.T, eps) @ (-1 * Dfx.T)\n",
    "        opt_alpha = secant(.9, 1.1, eps, f_alpha_prime)\n",
    "        x1 = x0 - opt_alpha * Dfx\n",
    "        err = np.linalg.norm(Dfx,2)\n",
    "        x0 = x1\n",
    "        iters += 1\n",
    "    return x0\n",
    "x0 = np.array([-1.,1.5])\n",
    "steepest_descent(rosenbrock,x0,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
